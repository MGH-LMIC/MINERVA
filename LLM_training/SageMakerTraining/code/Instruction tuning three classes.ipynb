{
 "cells": [
  {
   "cell_type": "code",
   "id": "3894a29d",
   "metadata": {},
   "source": [
    "# You only need to run this once per machine\n",
    "!pip install -q -U bitsandbytes \n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git \n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U datasets scipy ipywidgets\n",
    "!pip install urllib\n",
    "!pip install s3fs --upgrade\n",
    "!pip install botocore --upgrade\n",
    "!pip install trl"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d018f0a5",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3861c23e",
   "metadata": {},
   "source": "!huggingface-cli login --token \"[Your Token]\"",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f2181724",
   "metadata": {},
   "source": [
    "### Select base model"
   ]
  },
  {
   "cell_type": "code",
   "id": "6b90a5c5",
   "metadata": {},
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "augmentation_type = 'LLM'\n",
    "instruction_base_model = True\n",
    "full_instruct_model = False\n",
    "response_template = \"[/INST]\"\n",
    "save_model = False\n",
    "final_model = True\n",
    "k = 0\n",
    "hub_name = 'no'\n",
    "\n",
    "base_model = 'biomistralFINLLM'\n",
    "\n",
    "model_id = 'salangarica/BioMistral-LLM'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f714fb7c",
   "metadata": {},
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1f6cf4df",
   "metadata": {},
   "source": [
    "### Define templates"
   ]
  },
  {
   "cell_type": "code",
   "id": "c3bf5388",
   "metadata": {},
   "source": [
    "template_general_instruction = \"\"\"You are an expert microbiologist who given an excerpt from a research paper can easily \n",
    "identify the type of relation between a microbe and a disease. Doesn't create new information, but is completely faithful to the information provided, and always gives concise answers.\"\"\"\n",
    "\n",
    "template_instruction = \"\"\"Given the following meaning of the labels, answer the following question with the appropiate label.\n",
    "positive: This type is used to annotate microbe-disease entity pairs with positive correlation, such as microbe will cause or aggravate the disease, the microbe will increase when disease occurs.\n",
    "negative: This type is used to annotate microbe-disease entity pairs that have a negative correlation, such as microbe can be a treatment for a disease, or microbe will decrease when disease occurs. \n",
    "na: This type is used when the relation between a microbe and a disease is not clear from the context or there is no relation. In other words, use this label if the relation is not positive and not negative.\"\"\"\n",
    "\n",
    "template_evidence = \"\"\"Based on the above description, evidence is as follows: \n",
    "{evidence}\n",
    "\n",
    "What is the relationship between {microbe} and {disease}?\"\"\"\n",
    "\n",
    "template_system = template_general_instruction + '\\n' + template_instruction\n",
    "if 'mistral' in base_model and instruction_base_model:\n",
    "    template_user =  template_general_instruction + '\\n' + template_instruction + '\\n' + template_evidence\n",
    "else:\n",
    "    template_user = template_evidence\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3ec3fd72",
   "metadata": {},
   "source": [
    "### Getting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "ff60be38",
   "metadata": {},
   "source": [
    "import pandas as pd \n",
    "from datasets import Dataset\n",
    "\n",
    "def format_dataset(df):\n",
    "    formated_dataset = []\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        example_list = []\n",
    "        if 'mistral' in base_model and instruction_base_model:\n",
    "            example_list.append({'content': copy.deepcopy(template_user).format(evidence=row['EVIDENCE'],\n",
    "                                                                            microbe=row['MICROBE'],\n",
    "                                                                            disease=row['DISEASE']), 'role': 'user'})\n",
    "            example_list.append({'content': row['RELATION'], 'role': 'assistant'})\n",
    "        else:\n",
    "            example_list.append({'content': template_system, 'role': 'system'})\n",
    "            example_list.append({'content': copy.deepcopy(template_user).format(evidence=row['EVIDENCE'],\n",
    "                                                                            microbe=row['MICROBE'],\n",
    "                                                                            disease=row['DISEASE']), 'role': 'user'})\n",
    "            example_list.append({'content': row['RELATION'], 'role': 'assistant'})\n",
    "\n",
    "        formated_dataset.append({'message': example_list})\n",
    "    return formated_dataset\n",
    "\n",
    "if final_model:\n",
    "    if augmentation_type == 'RAG':\n",
    "        train_path = \"s3://finetune-mistral/instruction_finetuning/Final/AUG_RAG/train_k{}.csv\".format(k)\n",
    "        validation_path = \"s3://finetune-mistral/instruction_finetuning/Final/AUG_RAG/val_k{}.csv\".format(k)\n",
    "    elif augmentation_type == 'LLM':\n",
    "        train_path = \"s3://finetune-mistral/instruction_finetuning/Final/AUG_LLM/train_k{}.csv\".format(k)\n",
    "        validation_path = \"s3://finetune-mistral/instruction_finetuning/Final/AUG_LLM/val_k{}.csv\".format(k)\n",
    "    elif augmentation_type == 'ALL':\n",
    "        train_path = \"s3://finetune-mistral/instruction_finetuning/Final/ALL_AUG/train_k{}.csv\".format(k)\n",
    "        validation_path = \"s3://finetune-mistral/instruction_finetuning/Final/ALL_AUG/val_k{}.csv\".format(k)\n",
    "    else:\n",
    "        train_path = \"s3://finetune-mistral/instruction_finetuning/Final/NO_AUG/train_k{}.csv\".format(k)\n",
    "        validation_path = \"s3://finetune-mistral/instruction_finetuning/Final/NO_AUG/val_k{}.csv\".format(k)\n",
    "else:\n",
    "    if augmentation_type == 'RAG':\n",
    "        train_path = \"s3://finetune-mistral/instruction_finetuning/AUG_RAG/train_k{}.csv\".format(k)\n",
    "        validation_path = \"s3://finetune-mistral/instruction_finetuning/AUG_RAG/val_k{}.csv\".format(k)\n",
    "    elif augmentation_type == 'LLM':\n",
    "        train_path = \"s3://finetune-mistral/instruction_finetuning/AUG_LLM/train_k{}.csv\".format(k)\n",
    "        validation_path = \"s3://finetune-mistral/instruction_finetuning/AUG_LLM/val_k{}.csv\".format(k)\n",
    "    else:\n",
    "        train_path = \"s3://finetune-mistral/instruction_finetuning/train_k{}.csv\".format(k)\n",
    "        validation_path = \"s3://finetune-mistral/instruction_finetuning/val_k{}.csv\".format(k)\n",
    "\n",
    "\n",
    "training_data = pd.read_csv(train_path)\n",
    "validation_data = pd.read_csv(validation_path)\n",
    "\n",
    "training_data['RELATION'] = training_data['RELATION'].replace(['relate'], 'na')\n",
    "validation_data['RELATION'] = validation_data['RELATION'].replace(['relate'], 'na')\n",
    "\n",
    "\n",
    "print(training_data.head())\n",
    "print(training_data.shape)\n",
    "\n",
    "print(training_data.loc[training_data['RELATION'] == 'na'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e1515db0",
   "metadata": {},
   "source": [
    "### Format dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "08727f74",
   "metadata": {},
   "source": [
    "from datasets import Dataset\n",
    "import copy\n",
    "\n",
    "training_data = format_dataset(training_data)\n",
    "validation_data = format_dataset(validation_data)\n",
    "\n",
    "train_dataset = Dataset.from_list(training_data)\n",
    "validation_dataset = Dataset.from_list(validation_data)\n",
    "print(train_dataset)\n",
    "print(train_dataset[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9f5897b3",
   "metadata": {},
   "source": [
    "### Saving the processed dataset to disk"
   ]
  },
  {
   "cell_type": "code",
   "id": "cd7e78ee",
   "metadata": {},
   "source": [
    "if final_model:\n",
    "    if augmentation_type == 'RAG':\n",
    "        training_destination_path = f's3://{sess.default_bucket()}/instruction_finetuning/FINAL/processed_RAG/train3_classes_k{k}'\n",
    "        validation_destination_path = f's3://{sess.default_bucket()}/instruction_finetuning/FINAL/processed_RAG/validation3_classes_k{k}'\n",
    "    elif augmentation_type == 'LLM':\n",
    "        training_destination_path = f's3://{sess.default_bucket()}/instruction_finetuning/FINAL/processed_LLM/trtrain3_classesain_k{k}'\n",
    "        validation_destination_path = f's3://{sess.default_bucket()}/instruction_finetuning/FINAL/processed_LLM/validation3_classes_k{k}'\n",
    "    else:\n",
    "        training_destination_path = f's3://{sess.default_bucket()}/instruction_finetuning/FINAL/processed/train3_classes_k{k}'\n",
    "        validation_destination_path = f's3://{sess.default_bucket()}/instruction_finetuning/FINAL/processed/validation3_classes_k{k}'\n",
    "\n",
    "else:\n",
    "    if augmentation_type == 'RAG':\n",
    "        training_destination_path = f's3://{sess.default_bucket()}/instruction_finetuning/processed_RAG/train3_classes_k{k}'\n",
    "        validation_destination_path = f's3://{sess.default_bucket()}/instruction_finetuning/processed_RAG/validation3_classes_k{k}'\n",
    "    elif augmentation_type == 'LLM':\n",
    "        training_destination_path = f's3://{sess.default_bucket()}/instruction_finetuning/processed_LLM/trtrain3_classesain_k{k}'\n",
    "        validation_destination_path = f's3://{sess.default_bucket()}/instruction_finetuning/processed_LLM/validation3_classes_k{k}'\n",
    "    else:\n",
    "        training_destination_path = f's3://{sess.default_bucket()}/instruction_finetuning/processed/train3_classes_k{k}'\n",
    "        validation_destination_path = f's3://{sess.default_bucket()}/instruction_finetuning/processed/validation3_classes_k{k}'\n",
    "\n",
    "\n",
    "\n",
    "train_dataset.save_to_disk(training_destination_path)\n",
    "validation_dataset.save_to_disk(validation_destination_path)\n",
    "\n",
    "print('Training Saved to: {}'.format(training_destination_path))\n",
    "print('Validation Saved to: {}'.format(validation_destination_path))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c4ef6d3c",
   "metadata": {},
   "source": [
    "### Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "id": "cc81e24f",
   "metadata": {},
   "source": [
    "from huggingface_hub import HfFolder\n",
    "\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  'val_dataset_path':'/opt/ml/input/data/validation',\n",
    "  'hub_name':hub_name,\n",
    "    'save_model':save_model,\n",
    "    'num_train_epochs': 20,                            # number of training epochs\n",
    "  'instruction_base_model':instruction_base_model,\n",
    "    'full_instruct_model':full_instruct_model,\n",
    "    'response_template':response_template,\n",
    "    'per_device_train_batch_size': 8,                 # batch size for training\n",
    "  'gradient_accumulation_steps': 1,                 # Number of updates steps to accumulate \n",
    "  'gradient_checkpointing': True,                   # save memory but slower backward pass\n",
    "  'bf16': True,                                     # use bfloat16 precision\n",
    "  'tf32': True,                                     # use tf32 precision\n",
    "    'learning_rate': 2e-5,                            # learning rate\n",
    "  #'max_grad_norm': 0.3,                             # Maximum norm (for gradient clipping)\n",
    "  'warmup_ratio': 0.03,                             # warmup ratio\n",
    "  \"lr_scheduler_type\":\"constant\",                   # learning rate scheduler\n",
    "    'save_strategy': \"steps\",\n",
    "    'evaluation_strategy': \"steps\", \n",
    "    'save_steps':0.05,\n",
    "    'eval_steps':0.05,\n",
    "  \"logging_steps\": 10,                              # log every x steps\n",
    "  'merge_adapters': True,                           # wether to merge LoRA into the model (needs more memory)\n",
    "  'use_flash_attn': True,                           # Whether to use Flash Attention\n",
    "  'output_dir': '/tmp/run',                         # output directory, where to save assets during training                                                     # could be used for checkpointing. The final trained                                                    # model will always be saved to s3 at the end of training \n",
    "'load_best_model_at_end':True,\n",
    "    'save_total_limit':1,\n",
    "\n",
    "}\n",
    "\n",
    "if HfFolder.get_token() is not None:\n",
    "    hyperparameters['hf_token'] = HfFolder.get_token()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "17ff3309",
   "metadata": {},
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = '{}-3class-k{}'.format(base_model, k)\n",
    "\n",
    "print(job_name)\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'train_aws.py',    # train script\n",
    "    source_dir           = 'scripts',      # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.12xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    max_run              = 2*24*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    "    disable_output_compression = True         # not compress output to save training time and cost\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "73e03d7c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_destination_path, 'validation': validation_destination_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "15353024",
   "metadata": {},
   "source": [
    "from sagemaker import TrainingJobAnalytics\n",
    "\n",
    "training_job_name = huggingface_estimator.latest_training_job.job_name\n",
    "\n",
    "df = TrainingJobAnalytics(training_job_name=training_job_name).dataframe()\n",
    "df.head(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "557da2d5",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
